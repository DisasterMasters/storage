{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "                    #Mongo DB Collection Descriptions#\n",
    "##############################################################\n",
    "##############################################################\n",
    "'''\n",
    "LabeledStatuses_Hurricane_C   ()\n",
    "LabeledStatuses_Irma1_C   ()\n",
    "LabeledStatuses_Irma2_C   ()\n",
    "LabeledStatuses_MiscRelevant_C   ()\n",
    "LabeledStatuses_MiscTechCompanies_C   ()\n",
    "LabeledStatuses_Power_A   ()\n",
    "LabeledStatuses_Sandy_K   ()\n",
    "Statuses_Florence_A   ()\n",
    "Statuses_Florence_C   ()\n",
    "Statuses_Irma_A   ()\n",
    "Statuses_Irma_C   ()\n",
    "Statuses_Irma_K_26_media_subset   (subset of Doina's set of Irma tweets from the streaming API restrincted to tweets @ or \n",
    "                                                                            mentioning one of the 26 local media accounts)\n",
    "Statuses_Irma_K_w_usertype   (Doina's set of Irma tweets from the streaming API including a field that has the predicted \n",
    "                                                                                      usertype from the Classification ML)\n",
    "#do this one# Statuses_Irma_Scraped_Streamed   (Conbination of Statuses_Irma_K_26_media_subset and Tweets_Irma_Scraped with added fields \n",
    "                                                                to distinguish from which collection the tweet comes from)\n",
    "Statuses_Maria_A   ()\n",
    "Statuses_Maria_C   ()\n",
    "Statuses_MiscClimateChange_A   ()\n",
    "Statuses_MiscGlobal_A   ()\n",
    "Statuses_MiscKnoxville_A   ()\n",
    "Statuses_MiscPower_A   ()\n",
    "Tweets_Irma_Scraped   (set of Irma tweets collected from Scrapy Tweet Crawler, based on having to do with one of the 26 \n",
    "                                                                                                    local media accounts)\n",
    "\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set from Collection\n",
      "ID                                           913915921810456578\n",
      "_id                                    5cdee0933339f6a5e5b23ba1\n",
      "datetime                                    2017-09-29 23:58:30\n",
      "has_media                                                   NaN\n",
      "is_reply                                                   True\n",
      "is_retweet                                                False\n",
      "medias                                                      NaN\n",
      "nbr_favorite                                                  1\n",
      "nbr_reply                                                     0\n",
      "nbr_retweet                                                   0\n",
      "scraped                                                       1\n",
      "streamed                                                      0\n",
      "text                        So glad you were able to get there!\n",
      "tweeting_to_username                                 10NewsWTSP\n",
      "url                     /donna_baznik/status/913915921810456578\n",
      "user_id                                              3291252101\n",
      "usernameTweet                                      donna_baznik\n",
      "Name: 0, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change the settings block and run then go to second block\n",
    "\n",
    "                    #Settings#\n",
    "##############################################################\n",
    "##############################################################\n",
    "\n",
    "#name of the collection on mongodb under twitter\n",
    "collection_name = 'Tweets_Irma_Scraped'   \n",
    "\n",
    "#number of tweets to comprise the test set at most\n",
    "num = 1000\n",
    "\n",
    "#the name of the .xlsx you want to create\n",
    "xlsx_name = 'test_Irma_Scraped_urlextract.xlsx'\n",
    "\n",
    "#set to true if you want the test set to be comprised of only relevant tweets\n",
    "remove_irrelevant = False\n",
    "\n",
    "#set to true if you want the test set to only unique tweets\n",
    "remove_duplicates = True\n",
    "\n",
    "#set to true if you want the test set to not contain retweets\n",
    "remove_retweets = True\n",
    "\n",
    "#set to true if you want the test set to be comprised of only tweets in English\n",
    "make_english_only = False\n",
    "\n",
    "#set to true if you want the test set to be comprised of only tweets during the specified dates\n",
    "make_only_in_time_frame = False\n",
    "#date format '2019-01-01'\n",
    "date_begin = '2017-09-01'\n",
    "date_end = '2017-09-30'\n",
    "\n",
    "#set to None if not needed\n",
    "#otherwise set to list of keywords you want each tweet to contain one of ['governor', 'FEMA']\n",
    "only_contain_keywords = None #['governor', 'FEMA', 'fema', 'Fema']\n",
    "\n",
    "#set to None if not needed\n",
    "#otherwise set to the name of the csv that the training set is comprised of and that you dont want overlapping tweets from\n",
    "training_set = None #'opinion_new_train.csv'\n",
    "\n",
    "#set to False if you want tweets from news media sources\n",
    "#and set to True if you do not want them\n",
    "remove_tweets_from_media = False\n",
    "\n",
    "##############################################################\n",
    "##############################################################\n",
    "import sys\n",
    "if '/home/nwest13/twitter/Zach/Tool_Box/Modules' not in sys.path:\n",
    "    sys.path.insert(0, '/home/nwest13/twitter/Zach/Tool_Box/Modules')\n",
    "from Text_Cleaner import tc\n",
    "import Quick_Find\n",
    "import csv\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from bson import ObjectId\n",
    "from dateutil import parser\n",
    "import random\n",
    "import pickle\n",
    "import gensim\n",
    "from pd_doc2vec import doc2vec\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import *\n",
    "\n",
    "tc_class = tc()\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('/home/nwest13/twitter/Zach/Tool_Box/Relevant_Model/Relevance.model')\n",
    "\n",
    "client = pymongo.MongoClient('da1.eecs.utk.edu')\n",
    "if client['twitter'][collection_name].count_documents({}) < 1000000:\n",
    "    coll = list(client['twitter'][collection_name].find())\n",
    "else:\n",
    "    coll = Quick_Find.get_df(client['twitter'][collection_name])\n",
    "\n",
    "df_new_set = pd.DataFrame(columns=['Date', 'ID', 'Text'])\n",
    "df_coll_set = pd.DataFrame(coll) \n",
    "\n",
    "### TESTING\n",
    "df_coll_set = df_coll_set.head(5000)\n",
    "###\n",
    "\n",
    "if collection_name != \"Statuses_Irma_Scraped_Streamed\":\n",
    "    remove_tweets_from_media = False\n",
    "elif remove_tweets_from_media:\n",
    "    df_coll_set = df_coll_set[df_coll_set['from_media'] == 0]\n",
    "    \n",
    "if training_set != None:\n",
    "    df_training_set = pd.read_csv(training_set)\n",
    "    print('Training Set')\n",
    "    print(df_training_set.iloc[0])\n",
    "    print()\n",
    "    \n",
    "print('Set from Collection')    \n",
    "print(df_coll_set.iloc[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "Cleaning Text from Collection Set\n",
      "Removing retweets\n",
      "Removing any tweets not in english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [04:21<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the Data Frame after is: 4158\n",
      "\n",
      "Removing irrelevant tweets\n",
      "           text Is_Relevant_Or_Not\n",
      "0  Not Relevant         irrelevant\n",
      "1      Relevant           relevant\n",
      "predicting relevance\n",
      "798\n",
      "\n",
      "Removing retweets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cce06461be24129a1a8eb76cf289e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=798), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "798\n",
      "\n",
      "Removing duplicate tweets\n",
      "728\n",
      "\n",
      "728\n",
      "728\n",
      "Creating csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75c9e840d724128a5854d86781aeffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=728), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of tweets in the csv is 727\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#look at the print above and find the 4 fields that corespond to this data\n",
    "#replace the strings below with the correct name in the data frame\n",
    "##############################################################\n",
    "##############################################################\n",
    "#### For Collection Set ####\n",
    "Date = 'datetime'  #the date of the tweet\n",
    "Text = 'text'  #the tweets text\n",
    "ID = 'ID'  #the tweets ID\n",
    "\n",
    "#### For Training Set ####\n",
    "Train_set_Text = 'Text' #the tweets text \n",
    "##############################################################\n",
    "##############################################################\n",
    "\n",
    "def conv_date_seconds( string ): \n",
    "    date = parser.parse(string)\n",
    "    str_date = date.strftime(\"%Y-%m-%d\")\n",
    "    return int((datetime.strptime( str_date, \"%Y-%m-%d\" )).timestamp())\n",
    "\n",
    "print(len(df_coll_set))\n",
    "print()\n",
    "\n",
    "print('Cleaning Text from Collection Set')\n",
    "if remove_retweets == True and make_english_only == True:\n",
    "    print('Removing retweets')\n",
    "    print('Removing any tweets not in english')\n",
    "    df_coll_set = tc_class.clean_df(df_coll_set, Text, ['english', 'retweet', 'dot', 'at_user', 'hashtag', \n",
    "                                                        'html_link', 'pic_link', 'lower', 'numbers', 'punc',\n",
    "                                                        'stop_words', 'new_line']) \n",
    "    \n",
    "if remove_retweets == False and make_english_only == True:\n",
    "    print('Removing any tweets not in english')\n",
    "    df_coll_set = tc_class.clean_df(df_coll_set, Text, ['english', 'dot', 'at_user', 'hashtag',  \n",
    "                                                        'html_link', 'pic_link', 'lower', 'numbers', 'punc',\n",
    "                                                        'stop_words', 'new_line'])\n",
    "\n",
    "if remove_retweets == True and make_english_only == False:\n",
    "    print('Removing retweets')\n",
    "    df_coll_set = tc_class.clean_df(df_coll_set, Text, ['retweet', 'dot', 'at_user', 'hashtag',  \n",
    "                                                        'html_link', 'pic_link', 'lower', 'numbers', 'punc',\n",
    "                                                        'stop_words', 'new_line']) \n",
    "    \n",
    "if remove_retweets == False and make_english_only == False:\n",
    "    df_coll_set = tc_class.clean_df(df_coll_set, Text, ['dot', 'at_user', 'hashtag',  \n",
    "                                                        'html_link', 'pic_link', 'lower', 'numbers', 'punc',\n",
    "                                                        'stop_words', 'new_line'])\n",
    "\n",
    "df_coll_set.reset_index(drop=True, inplace=True)\n",
    "print()\n",
    "\n",
    "#Removes tweets in collection set that dont contain keywords\n",
    "if only_contain_keywords != None:\n",
    "    remove_list = []\n",
    "    print('Removing tweets that dont contain keywords')\n",
    "    \n",
    "    regex_keyword = re.compile(r\"(\" + r\"|\".join(only_contain_keywords) + \")\")    \n",
    "    df_temp = df_coll_set[Text].str.contains(regex_keyword)\n",
    "    df_coll_set = df_coll_set[df_temp]\n",
    "   \n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "\n",
    "#does not add irrelevant tweets to test set if set to True\n",
    "if remove_irrelevant == True:\n",
    "    print('Removing irrelevant tweets')\n",
    "    \n",
    "    df_relevant_train = pd.DataFrame(columns=['text', 'Is_Relevant_Or_Not'])\n",
    "\n",
    "    df_relevant_train.at[0, 'text'] = \"Not Relevant\"\n",
    "    df_relevant_train.at[0, 'Is_Relevant_Or_Not'] = \"irrelevant\"\n",
    "    df_relevant_train.at[1, 'text'] = \"Relevant\"\n",
    "    df_relevant_train.at[1, 'Is_Relevant_Or_Not'] = \"relevant\"\n",
    "\n",
    "    print(df_relevant_train)\n",
    "\n",
    "    # for i in df_coll_set.index:\n",
    "    #     df_coll_set.at[i, 'Is_Relevant_Or_Not'] = \"\"\n",
    "\n",
    "    x = doc2vec(df_relevant_train, 'text', ['Is_Relevant_Or_Not'], build=True, given=model)\n",
    "\n",
    "    print('predicting relevance')\n",
    "    df_coll_set['Is_Relevant_Or_Not'] = df_coll_set[Text].map(x.predict_text)    \n",
    "    df_coll_set['Is_Relevant_Or_Not'] = df_coll_set['Is_Relevant_Or_Not'].apply(lambda x: x[0])  \n",
    "    df_coll_set = df_coll_set[df_coll_set['Is_Relevant_Or_Not'] == 'relevant']\n",
    "    \n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "\n",
    "    \n",
    "#Removes retweets in collection set\n",
    "if remove_retweets == True:\n",
    "    print('Removing retweets')\n",
    "    remove_list = []\n",
    "    \n",
    "    for i in tqdm(df_coll_set.index):\n",
    "        if df_coll_set.at[i, Text].startswith('RT @'):\n",
    "            remove_list.append(i)\n",
    "        \n",
    "    df_coll_set = df_coll_set.drop(remove_list)\n",
    "    df_coll_set.reset_index(drop=True, inplace=True)   \n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "\n",
    "#Removes tweets in traing set from collection set if not set to None\n",
    "if training_set != None:\n",
    "    print('Removing tweets from collection set that are in Training Set')\n",
    "    df_training_set[Train_set_Text] = df_training_set[Train_set_Text].astype(str)\n",
    "    df_training_set = df_training_set[df_training_set[Train_set_Text] != 'nan']\n",
    "    df_training_set.reset_index(inplace=True, drop=True)\n",
    "    print()\n",
    "    print('Cleaning Text from Training Set')\n",
    "    for i in tqdm(df_training_set.index):\n",
    "        df_training_set.at[i, 'cleaned_text'] = tc_class.clean(df_training_set.at[i, Train_set_Text], ['dot', 'at_user', 'hashtag', \n",
    "                                                                             'punc', 'html_link', 'pic_link', 'lower', \n",
    "                                                                             'numbers', 'stop_words', 'new_line'])\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    remove_set = set()\n",
    "    print('Checking for overlap')\n",
    "    for i in tqdm(df_training_set.index):\n",
    "        remove_indices = df_coll_set[df_coll_set['cleaned_text'] == df_training_set.at[i, 'cleaned_text']].index \n",
    "        if len(remove_indices) > 0:\n",
    "            for one in remove_indices:\n",
    "                remove_set.add(one)\n",
    "      \n",
    "    print('Removing tweets')\n",
    "    df_coll_set = df_coll_set.drop(list(remove_set))\n",
    "    df_coll_set.reset_index(drop=True, inplace=True)\n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "\n",
    "#does not add duplicate tweets to test set if set to False\n",
    "if remove_duplicates == True:\n",
    "    print('Removing duplicate tweets')\n",
    "    df_coll_set = df_coll_set.drop_duplicates(subset='cleaned_text')\n",
    "    df_coll_set.reset_index(drop=True, inplace=True)\n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "\n",
    "#does not add tweets outside of time frame to the test set if set to True\n",
    "if make_only_in_time_frame == True:\n",
    "    print('Removing tweets not during time interval ' + date_begin + \" -- \" + date_end)\n",
    "    df_coll_set['seconds'] = df_coll_set[Date].map(conv_date_seconds)\n",
    "\n",
    "    #df holds seconds since epoch\n",
    "    #now convert 'to' and 'from' to seconds instead of '2017-09-28'\n",
    "\n",
    "    to_obj = datetime.strptime(date_end, '%Y-%m-%d')\n",
    "    from_obj = datetime.strptime(date_begin, '%Y-%m-%d')\n",
    "    date_end = int(to_obj.timestamp())\n",
    "    date_begin = int(from_obj.timestamp())\n",
    "\n",
    "    #filter df to seconds between to and from then drop seconds column\n",
    "\n",
    "    df_coll_set = df_coll_set[(df_coll_set['seconds'] <= date_end) & (df_coll_set['seconds'] >= date_begin)]\n",
    "    #df_coll_set = df_coll_set[df_coll_set['seconds'] == date_end]\n",
    "    df_coll_set.drop(columns=['seconds'], inplace=True)\n",
    "    df_coll_set.reset_index(drop=True, inplace=True)\n",
    "    print(len(df_coll_set))\n",
    "    print()\n",
    "    \n",
    "print(len(df_coll_set))\n",
    "df_coll_set = df_coll_set.drop([i for i in df_coll_set.index if df_coll_set.at[i, Date] == None])\n",
    "df_coll_set = df_coll_set.drop([i for i in df_coll_set.index if df_coll_set.at[i, Text] == None])\n",
    "df_coll_set = df_coll_set.drop([i for i in df_coll_set.index if df_coll_set.at[i, ID] == None])\n",
    "df_coll_set.reset_index(drop=True, inplace=True)\n",
    "print(len(df_coll_set))\n",
    "    \n",
    "list1=[]\n",
    "\n",
    "while 1:\n",
    "    if len(list1) == num or len(list1) == len(df_coll_set):\n",
    "        break\n",
    "    r = random.randint(0,len(df_coll_set))\n",
    "    if r not in list1: \n",
    "        list1.append(r)  \n",
    "        \n",
    "#make sure this prints out the number of desired tweets\n",
    "df_new_set['Date'] = None\n",
    "df_new_set['ID'] = None\n",
    "df_new_set['Text'] = None\n",
    "    \n",
    "k = 0\n",
    "print('Creating csv')\n",
    "for i in tqdm(df_coll_set.index):\n",
    "    if i in list1:\n",
    "        \n",
    "        date = parser.parse(df_coll_set.at[i, Date])\n",
    "        str_date = date.strftime(\"%Y-%m-%d\")\n",
    "        df_new_set.at[k, 'Date'] = str_date\n",
    "        df_new_set.at[k, 'ID'] = '=\"' + str(df_coll_set.at[i, ID]) + '\"'\n",
    "        df_new_set.at[k, 'Text'] = df_coll_set.at[i, Text]\n",
    "        k += 1\n",
    "        \n",
    "df_new_set.set_index('Date', inplace=True)\n",
    "df_new_set.to_excel(xlsx_name)\n",
    "\n",
    "print('Number of tweets in the csv is ' + str(len(df_new_set)))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
